# Local Development Configuration
# This config is specifically for Docker Compose local development

# Model List - Local Development
model_list:
  # Local Ollama model (Docker Compose service name)
  - model_name: llama3.2-3b
    litellm_params:
      model: ollama/llama3.2:3b
      api_base: http://ollama:11434  # Docker Compose service name, not localhost
      
  # Optional: Keep cloud models for testing (with your own API keys)
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY

# General Settings - Local Development
general_settings:
  # Local development master key
  master_key: sk-local-dev-key-12345
  
  # Local PostgreSQL connection (Docker Compose service name)
  database_url: postgresql://litellm_user:litellm_password@postgres:5432/litellm_db
  
  # Health check
  health_check: true
  
  # More verbose logging for development
  set_verbose: true
  json_logs: true

# Custom PII Detection Guardrails (same as production)
guardrails:
  # Fast regex-based PII detection
  - guardrail_name: "pii-regex-precall"
    litellm_params:
      guardrail: pii_regex_precall.PIIRegexPreCallGuardrail
      mode: "pre_call"
      
  - guardrail_name: "pii-regex-postcall"
    litellm_params:
      guardrail: pii_regex_postcall.PIIRegexPostCallGuardrail
      mode: "post_call"
  
  # Comprehensive ML-based PII detection using Microsoft Presidio
  - guardrail_name: "pii-presidio-precall"
    litellm_params:
      guardrail: pii_presidio_precall.PIIPresidioPreCallGuardrail
      mode: "pre_call"
      language: "en"
      threshold: 0.7
      block_on_detection: true
      
  - guardrail_name: "pii-presidio-postcall"
    litellm_params:
      guardrail: pii_presidio_postcall.PIIPresidioPostCallGuardrail
      mode: "post_call"
      language: "en"
      threshold: 0.7
      block_on_detection: true
